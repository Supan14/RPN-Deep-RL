{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt0mqyeavQ5L"
   },
   "source": [
    "#Tomato Dataset Bounding Box\n",
    "[Github repository](https://github.com/Ersy/object_detection_with_reinforcement_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEyXf3MLn4uO"
   },
   "source": [
    "# Image Augmentation\n",
    "### Take in an image and the bounding box/es, flip image horizontally determine new bounding  box locatiion based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MBYkMvcozFi"
   },
   "outputs": [],
   "source": [
    "# Image augmentation\n",
    "# take in an image and the bounding box/es\n",
    "# flip image horizontally\n",
    "# determine new bounding box location based on this\n",
    "\n",
    "# def flip_image(img, boundingbox):\n",
    "# \t\"\"\"\n",
    "# \tTakes an image and list of bounding boxes for the image \n",
    "# \tand flips everything horizontally\n",
    "# \treturns the flipped image and boundingbox \n",
    "# \t(elements of the bb are changed inplace)\n",
    "# \t\"\"\"\n",
    "# \tflipped_image = np.fliplr(img)\n",
    "# \timg_width = flipped_image.shape[1]\n",
    "# \tfor box_ix in range(len(boundingbox)):\n",
    "# \t\tbb_topx = boundingbox[box_ix][0, 1]\n",
    "# \t\tbb_bottomx = boundingbox[box_ix][1, 1]\n",
    "# \t\tbb_width = bb_bottomx - bb_topx\n",
    "\n",
    "# \t\tboundingbox[box_ix][0, 1] = img_width - bb_width - bb_topx\t\n",
    "# \t\tboundingbox[box_ix][1, 1] = img_width - bb_topx\n",
    "# \treturn flipped_image, boundingbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fomP7tTEogXy"
   },
   "source": [
    "# Image Actions\n",
    "### To load the images, to get image names, its labels and its ground truth bb, also doing image preprocessing and viewing its results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRZtydjIpphi"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "from keras.preprocessing import image\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naU-kWj1p8WL"
   },
   "outputs": [],
   "source": [
    "### Reference values\n",
    "class_name_dict = { \t\n",
    "                    'tomato':0,\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb_W4GtKqIAl"
   },
   "outputs": [],
   "source": [
    "def load_images(VOC_path, image_names):\n",
    "\t\"\"\"\n",
    "\tloads images from a given data set\n",
    "\t\"\"\"\n",
    "\timages = []\n",
    "\tfor i in range(len(image_names)):\n",
    "\t\timage_name = image_names[i]\n",
    "\t\tstring = VOC_path + '/JPEGImages/' + image_name + '.jpg'\n",
    "\t\timages.append(image.load_img(string, False))\n",
    "\treturn images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQpufecFqMpM"
   },
   "outputs": [],
   "source": [
    "def get_img_names(VOC_path, data_set_name):\n",
    "\t\"\"\"\n",
    "\tcollects the file names associated with a class and data set type\n",
    "\t\"\"\"\n",
    "\tfile_path = VOC_path + data_set_name + '.txt'\n",
    "\tf = open(file_path)\n",
    "\timage_names = f.readlines()\n",
    "\timage_names = [x.strip('\\n') for x in image_names]\n",
    "\tf.close()\n",
    "\treturn [x.split(None, 1)[0] for x in image_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86QbRXcSqSWO"
   },
   "outputs": [],
   "source": [
    "def get_img_labels(VOC_path, data_set_name):\n",
    "\t\"\"\"\n",
    "\tcollects the labels for the desired dataset\n",
    "\t\"\"\"\n",
    "\tfile_path = VOC_path + '/ImageSets/Main/' + data_set_name + '.txt'\n",
    "\tf = open(file_path)\n",
    "\timage_names = f.readlines()\n",
    "\timage_names = [x.strip('\\n') for x in image_names]\n",
    "\tf.close()\n",
    "\treturn [x.split(None, 1)[1] for x in image_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVCzzBSiqW3k"
   },
   "outputs": [],
   "source": [
    "# def get_bb_gt(VOC_path, image_name):\n",
    "# \t\"\"\"\n",
    "# \tget the ground truth bounding box values and class for an image\n",
    "# \t\"\"\"\n",
    "# \tfile_path = VOC_path + '/Annotations/' + image_name + '.xml'\n",
    "# \ttree = ET.parse(file_path)\n",
    "# \troot = tree.getroot()\n",
    "# \tnames = []\n",
    "# \tx_min = []\n",
    "# \tx_max = []\n",
    "# \ty_min = []\n",
    "# \ty_max = []\n",
    "# \tfor child in root:\n",
    "# \t\tif child.tag == 'object':\n",
    "# \t\t\tfor child2 in child:\n",
    "# \t\t\t\tif child2.tag == 'name':\n",
    "# \t\t\t\t\tnames.append(child2.text)\n",
    "# \t\t\t\telif child2.tag == 'bndbox':\n",
    "# \t\t\t\t\tfor child3 in child2:\n",
    "# \t\t\t\t\t\tif child3.tag == 'xmin':\n",
    "# \t\t\t\t\t\t\tx_min.append(child3.text)\n",
    "# \t\t\t\t\t\telif child3.tag == 'xmax':\n",
    "# \t\t\t\t\t\t\tx_max.append(child3.text)\n",
    "# \t\t\t\t\t\telif child3.tag == 'ymin':\n",
    "# \t\t\t\t\t\t\ty_min.append(child3.text)\n",
    "# \t\t\t\t\t\telif child3.tag == 'ymax':\n",
    "# \t\t\t\t\t\t\ty_max.append(child3.text)\n",
    "# \tbb_list = []\n",
    "# \tcategory = []\n",
    "# \tfor i in range(np.size(names)):\n",
    "# \t\tcategory.append(class_name_dict[names[i]])\n",
    "# \t\tbb_list.append(np.array([[y_min[i], x_min[i]],[y_max[i], x_max[i]]]))\n",
    "# \treturn np.array(category, dtype='uint16'), np.array(bb_list, dtype='uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBxVMr8oqiJU"
   },
   "outputs": [],
   "source": [
    "def view_image(t0):\n",
    "\t\"\"\"\n",
    "\tconverts an image back into a viewable format (PIL) and displays\n",
    "\t\"\"\"\n",
    "\tt0[:, :, 0] += 103\n",
    "\tt0[:, :, 1] += 116\n",
    "\tt0[:, :, 2] += 123\n",
    "\tt1 = np.uint8(t0)\n",
    "\tt2 = Image.fromarray(t1)\n",
    "\tt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5klsJMkqkpi"
   },
   "outputs": [],
   "source": [
    "def image_preprocessing(im):\n",
    "\t\"\"\"\n",
    "\tpreprocessing for images before VGG16\n",
    "\tchange the colour channel order\n",
    "\tresize to 224x224\n",
    "\tadd dimension for input to vgg16\n",
    "\tcarry out standard preprocessing\n",
    "\t\"\"\"\n",
    "\tim = im[:, :, ::-1] # keep this in if the color channel order needs reversing\n",
    "\tim = cv2.resize(im, (224, 224)).astype(np.float32)\n",
    "\tim = np.expand_dims(im, axis=0)\n",
    "\tim = preprocess_input(im)\n",
    "\treturn im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stj2UGnqqo8F"
   },
   "outputs": [],
   "source": [
    "def view_results(im, groundtruth, proposals, all_IOU, ix):\n",
    "\t\"\"\"\n",
    "\ttakes in an image set, ground truth bounding boxes, proposal bounding boxes, and an image index\n",
    "\tprints out the image with the bouning boxes drawn in\n",
    "\t\"\"\"\n",
    "\tim = im[ix]\n",
    "\tmax_IOU = max(all_IOU[ix][-1])\n",
    "\tproposals = proposals[ix]\n",
    "\n",
    "\tfig, ax = plt.subplots(1)\n",
    "\tax.imshow(im)\n",
    "\n",
    "\tnum_of_proposals = len(proposals)\n",
    "\tcolor = plt.cm.rainbow(np.linspace(0,1,num_of_proposals))\n",
    "\n",
    "\tfor proposal, c in zip(proposals, color):\n",
    "\t    top_left = (proposal[0,1], proposal[0,0])\n",
    "\t    width = proposal[1,1] - proposal[0,1]\n",
    "\t    height = proposal[1,0] - proposal[0,0]\n",
    "\t    rect = patches.Rectangle(top_left, width, height, linewidth=2, edgecolor=c, facecolor='none') # change facecolor to add fill\n",
    "\t    ax.add_patch(rect)\n",
    "\trect = patches.Rectangle(top_left, width, height, linewidth=2, edgecolor=c, facecolor='none' , label='Max IoU: '+str(max_IOU)[:5])\n",
    "\tax.add_patch(rect)\n",
    "\n",
    "\tfor ground_truth_box in groundtruth[ix]:\n",
    "\t    top_left = (ground_truth_box[0,1], ground_truth_box[0,0])\n",
    "\t    width = ground_truth_box[1,1] - ground_truth_box[0,1]\n",
    "\t    height = ground_truth_box[1,0] - ground_truth_box[0,0]\n",
    "\t    rect = patches.Rectangle(top_left, width, height, linewidth=2, edgecolor='white', facecolor='none')\n",
    "\t    ax.add_patch(rect)\n",
    "\n",
    "\n",
    "\tplt.legend()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2IG-qV6rFWz"
   },
   "source": [
    "# Image Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHtpn7zorP7u"
   },
   "outputs": [],
   "source": [
    "def get_class_images(VOC_path, desired_class, img_name_list, img_list):\n",
    "\n",
    "\t# collect the code for desired object class\n",
    "\tdesired_class = image_actions.class_name_dict[desired_class]\n",
    "\n",
    "\tdesired_class_list_bb = []\n",
    "\tdesired_class_list_image = []\n",
    "\tdesired_class_list_name = []\n",
    "\n",
    "\t# collect bounding boxes for each image\n",
    "\tfor image_ix in range(len(img_name_list)):\n",
    "\t\tcurrent_image_groundtruth = []\n",
    "\t\tground_image_bb_gt = image_actions.get_bb_gt(VOC_path, img_name_list[image_ix])\n",
    "\t\t\n",
    "\t\t# flag the image as containing the desired target object\n",
    "\t\timage_flag = False\t\n",
    "\t\tfor ix in range(len(ground_image_bb_gt[0])):\t\n",
    "\t\t\tif ground_image_bb_gt[0][ix] == desired_class:\n",
    "\t\t\t\tcurrent_image_groundtruth.append(ground_image_bb_gt[1][ix])\n",
    "\t\t\t\timage_flag = True\n",
    "\n",
    "\t\t# append images that contain desired object\n",
    "\t\tif image_flag:\n",
    "\t\t\tdesired_class_list_bb.append(current_image_groundtruth)\t\n",
    "\t\t\tdesired_class_list_image.append(img_list[image_ix])\n",
    "\t\t\tdesired_class_list_name.append(img_name_list[image_ix])\n",
    "\n",
    "\treturn desired_class_list_image, desired_class_list_bb, desired_class_list_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVXVAIF0rcO6"
   },
   "source": [
    "# Action Functions\n",
    "\n",
    "### Performing right, down, left and up actions and updating the bb information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgqAjENArbBJ"
   },
   "outputs": [],
   "source": [
    "# dictionary mapping Q output index to actions\n",
    "action_dict = {0:'right',1:'down',2:'left',3:'up'}\n",
    "\n",
    "# amount to update the corner positions by for each step\n",
    "update_step = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgHADoCMsCnJ"
   },
   "outputs": [],
   "source": [
    "def TL_right(bb):\n",
    "\t\"\"\"moves the top corner to the right\"\"\"\n",
    "\ty_origin = bb[0,0]\n",
    "\tx_origin = bb[0,1]\n",
    "\t\n",
    "\ty_end = bb[1,0]\n",
    "\tx_end = bb[1,1]\n",
    "\n",
    "\tpixel_update = int((x_end - x_origin) * update_step)\n",
    "\n",
    "\tx_origin = x_origin + pixel_update\n",
    "\n",
    "\ttl = [y_origin, x_origin]\n",
    "\tbr = [y_end, x_end]\n",
    "\treturn np.array([tl, br])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnQQv1FYsE6T"
   },
   "outputs": [],
   "source": [
    "def TL_down(bb):\n",
    "\t\"\"\"moves the top corner to the right\"\"\"\n",
    "\ty_origin = bb[0,0]\n",
    "\tx_origin = bb[0,1]\n",
    "\t\n",
    "\ty_end = bb[1,0]\n",
    "\tx_end = bb[1,1]\n",
    "\n",
    "\tpixel_update = int((y_end - y_origin) * update_step)\n",
    "\n",
    "\ty_origin = y_origin + pixel_update\n",
    "\n",
    "\ttl = [y_origin, x_origin]\n",
    "\tbr = [y_end, x_end]\n",
    "\treturn np.array([tl, br])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfZ-boDEsHLa"
   },
   "outputs": [],
   "source": [
    "def BR_left(bb):\n",
    "\t\"\"\"moves the bottom corner to the left\"\"\"\n",
    "\ty_origin = bb[0,0]\n",
    "\tx_origin = bb[0,1]\n",
    "\t\n",
    "\ty_end = bb[1,0]\n",
    "\tx_end = bb[1,1]\n",
    "\n",
    "\tpixel_update = int((x_end - x_origin) * update_step)\n",
    "\n",
    "\tx_end = x_end - pixel_update\n",
    "\n",
    "\ttl = [y_origin, x_origin]\n",
    "\tbr = [y_end, x_end]\n",
    "\treturn np.array([tl, br])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGomWR5UsJKS"
   },
   "outputs": [],
   "source": [
    "def BR_up(bb):\n",
    "\t\"\"\"moves the top corner to the right\"\"\"\n",
    "\ty_origin = bb[0,0]\n",
    "\tx_origin = bb[0,1]\n",
    "\t\n",
    "\ty_end = bb[1,0]\n",
    "\tx_end = bb[1,1]\n",
    "\n",
    "\tpixel_update = int((y_end - y_origin) * update_step)\n",
    "\n",
    "\ty_end = y_end - pixel_update\n",
    "\n",
    "\ttl = [y_origin, x_origin]\n",
    "\tbr = [y_end, x_end]\n",
    "\treturn np.array([tl, br])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJdqvZd-sMYZ"
   },
   "outputs": [],
   "source": [
    "def crop_image(im, bb_in, region):\n",
    "\t\"\"\"\n",
    "\treturns a desired cropped region of the raw image\n",
    "\tim: raw image (numpy array)\n",
    "\tbb: the bounding box of the current region (defined by top left and bottom right corner points)\n",
    "\tregion: 'TL', 'TR', 'BL', 'BR', 'centre'\n",
    "\t\"\"\"\n",
    "\n",
    "\tif action_dict[region] == 'right':\n",
    "\t\tnew_bb = TL_right(bb_in)\n",
    "\telif action_dict[region] == 'down':\n",
    "\t\tnew_bb = TL_down(bb_in)\n",
    "\telif action_dict[region] == 'left':\n",
    "\t\tnew_bb = BR_left(bb_in)\n",
    "\telif action_dict[region] == 'up':\n",
    "\t\tnew_bb = BR_up(bb_in)\n",
    "\n",
    "\ty_start = new_bb[0,0]\n",
    "\ty_end = new_bb[1,0]\n",
    "\tx_start = new_bb[0,1]\n",
    "\tx_end = new_bb[1,1]\n",
    "\n",
    "\t# crop image to new boundingbox extents\n",
    "\tim = im[int(y_start):int(y_end), int(x_start):int(x_end), :]\n",
    "\treturn im, new_bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKIl9mVGsUDc"
   },
   "source": [
    "# Gabor Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67pNM9EdsWN_"
   },
   "outputs": [],
   "source": [
    "# from scipy.ndimage import zoom\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import cPickle as pickle\n",
    "\n",
    "# from scipy.ndimage import zoom\n",
    "# import numpy as np\n",
    "# import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeZUNQjpsfYK"
   },
   "outputs": [],
   "source": [
    "# class gabor_gen():\n",
    "    \n",
    "#     def __init__(self, im_size):\n",
    "#         self.im_size = im_size\n",
    "        \n",
    "#     def gen_image(self, num_of_gabors, gabor_size, lambda_, theta,sigma, phase, \n",
    "#                   noisy=False, beta=-2, random_scaling=False, odd_one_out=False, \n",
    "#                   overlap=False, random_angles=False, occluded=False):\n",
    "#         \"\"\"\n",
    "#         Generates an image of a select size with a select number of gabors\n",
    "#         \"\"\"\n",
    "#         im_len = (np.linspace(0, self.im_size, self.im_size+1))\n",
    "#         x_mesh, y_mesh = np.meshgrid(im_len, im_len)\n",
    "#         bb = []\n",
    "        \n",
    "#         if noisy:\n",
    "#             # create spatial noise background and normalise\n",
    "#             im = self.spatial_noise(beta)\n",
    "#             im = (im - im.mean())/im.std()\n",
    "#         else:\n",
    "#             im = x_mesh*0 + y_mesh*0\n",
    "        \n",
    "        \n",
    "#         # collection of all coordinates in grid\n",
    "#         available_space = [(y,x) for y, x in zip(y_mesh.flatten(), x_mesh.flatten())]\n",
    "#         # storage for collecting gabor locations\n",
    "#         existing_gabor_loc = []\n",
    "#         # storage for collecting gabor sizes\n",
    "#         existing_gabor_size = []\n",
    "        \n",
    "#         # create gabor patches in the image\n",
    "#         for gab in range(num_of_gabors):\n",
    "            \n",
    "#             # hack to make the last gabor angle perpendicular to the rest\n",
    "#             if odd_one_out and gab == 1:\n",
    "#                 theta = theta+90\n",
    "            \n",
    "#             # allow for random angle generation\n",
    "#             if random_angles:\n",
    "#                 theta = random.choice([0,45,90,135,180,225,270,315])\n",
    "            \n",
    "#             # flag for random scaling of patches for variability\n",
    "#             scaling_factor = 1\n",
    "#             if random_scaling:\n",
    "#                 scaling_factor = random.randint(1,3)\n",
    "\n",
    "            \n",
    "#             # create gabor and normalise\n",
    "#             gabor, gauss = self.gabor_patch(size=gabor_size, lambda_=lambda_,theta=theta,sigma=sigma, phase=phase)\n",
    "#             gabor = zoom(gabor, scaling_factor)\n",
    "#             gauss = zoom(gauss, scaling_factor)\n",
    "#             gabor = (gabor - gabor.mean())/gabor.std()\n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "#             # get the scaled gabor size\n",
    "#             scaled_gabor_size = gabor_size*scaling_factor\n",
    "            \n",
    "#             available_space = [(y, x) for y, x in available_space if x < self.im_size-scaled_gabor_size and y < self.im_size-scaled_gabor_size]\n",
    "#             # generate a random location to place the new gabor\n",
    "#             #x, y = self.gen_random_location(im_len, scaled_gabor_size, existing_gabor_size, existing_gabor_loc, overlap)\n",
    "#             if available_space:\n",
    "#                 x, y = self.gen_random_location(available_space)\n",
    "#                 x, y = int(x), int(y)\n",
    "                \n",
    "#                 if overlap == False:\n",
    "#                     available_space = self.get_available_space(available_space, x, y, scaled_gabor_size, im_len)\n",
    "\n",
    "                    \n",
    "                \n",
    "#                 x_min = x\n",
    "#                 y_min = y\n",
    "#                 x_max = x+scaled_gabor_size\n",
    "#                 y_max = y+scaled_gabor_size\n",
    "                \n",
    "#                 if occluded:\n",
    "#                     half_y = y+int(scaled_gabor_size/2)\n",
    "#                     half_x = x+int(scaled_gabor_size/2)\n",
    "                \n",
    "#                     random_occlusion_x = random.randint(0,2)\n",
    "#                     if random_occlusion_x == 0:\n",
    "#                         x_min = half_x\n",
    "#                     elif random_occlusion_x == 1:\n",
    "#                         x_max = half_x\n",
    "\n",
    "#                     # trick to prevent full patches from being created\n",
    "#                     y_occ = (1 if random_occlusion_x == 2 else 2)\n",
    "\n",
    "#                     random_occlusion_y = random.randint(0,y_occ)\n",
    "#                     if random_occlusion_y == 0:\n",
    "#                         y_min = half_y\n",
    "#                     elif random_occlusion_y == 1:\n",
    "#                         y_max = half_y                    \n",
    "                    \n",
    "                \n",
    "#                     im[y_min:y_max,x_min:x_max] = im[y_min:y_max,x_min:x_max]*(1-gauss[0+y_min-y:y_max-y, 0+x_min-x:x_max-x])\n",
    "#                     im[y_min:y_max,x_min:x_max] = im[y_min:y_max,x_min:x_max]+gabor[0+y_min-y:y_max-y, 0+x_min-x:x_max-x]\n",
    "                \n",
    "#                 else:\n",
    "#                     # reduce noise in the gabor region by 1-gaussian then add gabor patch\n",
    "#                     im[y:y+scaled_gabor_size,x:x+scaled_gabor_size] = im[y:y+scaled_gabor_size,x:x+scaled_gabor_size]*(1-gauss)\n",
    "#                     im[y:y+scaled_gabor_size,x:x+scaled_gabor_size] = im[y:y+scaled_gabor_size,x:x+scaled_gabor_size]+gabor\n",
    "\n",
    "\n",
    "\n",
    "#                 if occluded:\n",
    "#                     bb.append(np.array([[y_min, x_min],[y_max, x_max]]))\n",
    "#                 else:\n",
    "#                     bb.append(np.array([[y, x],[y+scaled_gabor_size, x+scaled_gabor_size]]))\n",
    "#             else:\n",
    "#                 print(\"No more space available after \"+ str(gab) + \" patches\")\n",
    "#                 break\n",
    "        \n",
    "#         if odd_one_out:\n",
    "#             bb = [bb[0]]\n",
    "        \n",
    "        \n",
    "#         # 0-255 mapping\n",
    "#         im = self._convert_to_im(im)\n",
    "            \n",
    "#         return im, bb\n",
    "    \n",
    "#     def _convert_to_im(self, im):\n",
    "#         \"\"\"\n",
    "#         converts image array values from original range to 0-255\n",
    "#         \"\"\"\n",
    "#         input_min = im.min()\n",
    "#         input_max = im.max()\n",
    "#         output_min = 0\n",
    "#         output_max = 255\n",
    "        \n",
    "#         input_range = input_max - input_min\n",
    "#         output_range = output_max - output_min\n",
    "\n",
    "#         new_im = ((im - input_min) * output_range / input_range) + output_min\n",
    "#         new_im = np.uint8(np.ceil(new_im))\n",
    "#         new_im = self.to_rgb1a(new_im)\n",
    "        \n",
    "#         return new_im\n",
    "\n",
    "#     def to_rgb1a(self, im):\n",
    "#         \"\"\"\n",
    "#         converts image from single channel to 3 channels\n",
    "#         code from: http://www.socouldanyone.com/2013/03/converting-grayscale-to-rgb-with-numpy.html (Matt Murfitt, 2013)\n",
    "#         \"\"\"\n",
    "#         w, h = im.shape\n",
    "#         ret = np.empty((w, h, 3), dtype=np.uint8)\n",
    "#         ret[:, :, 2] =  ret[:, :, 1] =  ret[:, :, 0] =  im\n",
    "#         return ret    \n",
    "    \n",
    "#     def gen_random_location(self, available_space):\n",
    "#         \"\"\"\n",
    "#         Selects a random location within the bounds of the image\n",
    "#         \"\"\"\n",
    "#         y, x = random.choice(available_space)\n",
    "        \n",
    "#         return x, y\n",
    "    \n",
    "    \n",
    "#     def get_available_space(self, available_space, x, y, scaled_gabor_size, im_len):\n",
    "#         \"\"\"\n",
    "#         update the available space list to remove the \n",
    "#         \"\"\"\n",
    "        \n",
    "        \n",
    "#         available_space = [(a,b) for a,b in available_space if ((a+scaled_gabor_size<y or b+scaled_gabor_size<x)\n",
    "#                                                                 or \n",
    "#                                                                 (a>y+scaled_gabor_size or b>x+scaled_gabor_size))]\n",
    "        \n",
    "        \n",
    "#         # get current available space to account for current gabor size hitting the edge\n",
    "#         current_x = [x for x in im_len][:-scaled_gabor_size]\n",
    "#         current_y = [y for y in im_len][:-scaled_gabor_size]\n",
    "#         current_grid = np.meshgrid(current_y, current_x)\n",
    "#         current_available_space = [(y,x) for y, x in zip(current_grid[0].flatten(), current_grid[1].flatten())]\n",
    "        \n",
    "#         available_space = list(set(available_space).intersection(current_available_space))\n",
    "        \n",
    "#         return available_space\n",
    "\n",
    "\n",
    "#     def spatial_noise(self, beta):\n",
    "#         \"\"\"\n",
    "#         generates a noisy background with a given power spectrum\n",
    "#         adapted from http://uk.mathworks.com/matlabcentral/fileexchange/5091-generate-spatial-data (Jon Yearsley, 2016)\n",
    "#         \"\"\"\n",
    "#         DIM = [self.im_size,self.im_size]\n",
    "#         BETA = beta\n",
    "\n",
    "#         u1 = np.array(range(0,int(DIM[0]/2)+1, 1))\n",
    "#         u2 = -np.array(range(int(np.ceil(DIM[0]/2))-1, 0, -1))\n",
    "#         u = (np.hstack((u1, u2))/DIM[0])\n",
    "#         u = np.tile(u, (DIM[1],1)).T\n",
    "\n",
    "\n",
    "#         v1 = np.array(range(0,int(DIM[1]/2.0)+1, 1))\n",
    "#         v2 = -np.array(range(int(np.ceil(DIM[1]/2.0))-1, 0, -1))\n",
    "#         v = (np.hstack((v1, v2))/DIM[1])\n",
    "#         v = np.tile(v, (DIM[0],1))\n",
    "\n",
    "#         Spatial_freq = np.power(np.power(u, 2) + np.power(v, 2), (BETA/2.0))\n",
    "\n",
    "#         Spatial_freq[Spatial_freq == np.inf] =0\n",
    "\n",
    "#         phi = np.random.rand(DIM[0], DIM[1])\n",
    "\n",
    "#         a = np.power(Spatial_freq, 0.5)\n",
    "#         b = (np.cos(2*np.pi*phi))+(1j*np.sin(2*np.pi*phi))\n",
    "\n",
    "#         x = np.fft.ifft2(a*b)\n",
    "#         im = np.real(x)\n",
    "#         return im\n",
    "        \n",
    "        \n",
    "#     def gabor_patch(self, size, lambda_, theta, sigma, phase, trim=.005):\n",
    "#         \"\"\"\n",
    "#         Create a Gabor Patch\n",
    "#         size : int\n",
    "#             Image size (n x n)\n",
    "#         lambda_ : int\n",
    "#             Spatial frequency (px per cycle)\n",
    "#         theta : int or float\n",
    "#             Grating orientation in degrees\n",
    "#         sigma : int or float\n",
    "#             gaussian standard deviation (in pixels)\n",
    "#         phase : float\n",
    "#             0 to 1 inclusive\n",
    "#         \"\"\"\n",
    "#         # make linear ramp\n",
    "#         X0 = (np.linspace(1, size, size) / size) - .5\n",
    "\n",
    "#         # Set wavelength and phase\n",
    "#         freq = size / float(lambda_)\n",
    "#         phaseRad = phase * 2 * np.pi\n",
    "\n",
    "#         # Make 2D grating\n",
    "#         Xm, Ym = np.meshgrid(X0, X0)\n",
    "\n",
    "#         # Change orientation by adding Xm and Ym together in different proportions\n",
    "#         thetaRad = (theta / 360.) * 2 * np.pi\n",
    "#         Xt = Xm * np.cos(thetaRad)\n",
    "#         Yt = Ym * np.sin(thetaRad)\n",
    "#         grating = np.sin(((Xt + Yt) * freq * 2 * np.pi) + phaseRad)\n",
    "\n",
    "#         # 2D Gaussian distribution\n",
    "#         gauss =  np.exp(-((Xm ** 2) + (Ym ** 2)) / (2 * (sigma / float(size)) ** 2))\n",
    "        \n",
    "#         # Trim\n",
    "#         cropped_gauss = gauss[gauss < trim] = 0\n",
    "\n",
    "#         return grating * gauss, gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaLAaeisqzJ"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.patches as patches\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# gabor_size=30\n",
    "# sigma=5\n",
    "# num_of_pics = 5\n",
    "# num_of_gabors = 1\n",
    "# im_size = 224\n",
    "# beta = -2\n",
    "# noisy=True\n",
    "# phase=0\n",
    "# lambda_ = 6\n",
    "# theta=0\n",
    "# random_scaling = False\n",
    "# odd_one_out = False\n",
    "# overlap=False\n",
    "# random_angles = False\n",
    "# occluded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5N7yUL4sr4P"
   },
   "outputs": [],
   "source": [
    "# def generate_x_images(num_of_pics, im_size, num_of_gabors, gabor_size, lambda_, theta, phase, sigma, \n",
    "#                       noisy, random_scaling, odd_one_out, overlap, random_angles, occluded):\n",
    "#     \"\"\"\n",
    "#     Generates multiple images with the same gabor settings\n",
    "#     \"\"\"\n",
    "#     image_container = []\n",
    "#     bb_container = []\n",
    "#     gabor_instance = gabor_gen(im_size=im_size)\n",
    "#     for i in range(num_of_pics):\n",
    "#         image, bb = gabor_instance.gen_image(num_of_gabors=num_of_gabors, \n",
    "#                                              gabor_size=gabor_size,\n",
    "#                                              lambda_ = lambda_,\n",
    "#                                              theta = theta,\n",
    "#                                              phase=phase,\n",
    "#                                              sigma=sigma, \n",
    "#                                              beta=beta, \n",
    "#                                              noisy=noisy,\n",
    "#                                              random_scaling=random_scaling,\n",
    "#                                              odd_one_out=odd_one_out,\n",
    "#                                              overlap=overlap,\n",
    "#                                              random_angles=random_angles,\n",
    "#                                              occluded=occluded)\n",
    "#         image_container.append(image)\n",
    "#         bb_container.append(bb)\n",
    "#     return image_container, bb_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72buGvzDs0jG"
   },
   "outputs": [],
   "source": [
    "# train_images, train_bbs = generate_x_images(num_of_pics, im_size, num_of_gabors, gabor_size, lambda_, theta, phase, sigma, \n",
    "#                                             noisy, random_scaling, odd_one_out, overlap, random_angles, occluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HG-noYJds8uc"
   },
   "source": [
    "# Reinforcement Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WO3S3Ffs_-x"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential # part to build the mode\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten # types of layers and associated functions\n",
    "from keras.optimizers import RMSprop, SGD, Nadam, Adam #optimising method (cost function and update method)\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "# from keras.initializers import normal, identity\n",
    "from keras.initializers import RandomNormal, Identity\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iY0nt3bttEcs"
   },
   "outputs": [],
   "source": [
    "# Visual descriptor size\n",
    "visual_descriptor_size = 25088\n",
    "# Different actions that the agent can do\n",
    "number_of_actions = 5\n",
    "\n",
    "# Number of actions in the past to retain\n",
    "past_action_val = 8\n",
    "\n",
    "movement_reward = 1\n",
    "\n",
    "\n",
    "terminal_reward_5 = 3\n",
    "terminal_reward_7 = 5\n",
    "terminal_reward_9 = 7\n",
    "\n",
    "iou_threshold_5 = 0.7\n",
    "iou_threshold_7 = 0.7\n",
    "iou_threshold_9 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJyW5Z-dtJnU"
   },
   "outputs": [],
   "source": [
    "def conv_net_out(image, model_vgg):\n",
    "\treturn model_vgg.predict(image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_8m6sAGtL_B"
   },
   "outputs": [],
   "source": [
    "### get the state by vgg_conv output, vectored, and stack on action history\n",
    "def get_state_as_vec(image, history_vector, model_vgg):\n",
    "\tdescriptor_image = conv_net_out(image, model_vgg)\n",
    "\tdescriptor_image = np.reshape(descriptor_image, (visual_descriptor_size, 1))\n",
    "\thistory_vector = np.reshape(history_vector, (number_of_actions*past_action_val, 1))\n",
    "\tstate = np.vstack((descriptor_image, history_vector)).T\n",
    "\treturn state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGqrbcn7tOp1"
   },
   "outputs": [],
   "source": [
    "def get_q_network(shape_of_input, number_of_actions, weights_path='0'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(1024, use_bias=True, kernel_initializer='lecun_uniform', input_shape = shape_of_input))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(1024, use_bias=True, kernel_initializer='lecun_uniform'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(number_of_actions, use_bias=True, kernel_initializer='lecun_uniform'))\n",
    "\tmodel.add(Activation('linear'))\n",
    "\tadam = Adam(lr=1e-6)\n",
    "\t#nadam = Nadam()\n",
    "\tmodel.compile(loss='mse', optimizer=adam)\n",
    "\tif weights_path != \"0\":\n",
    "\t\tmodel.load_weights(weights_path)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY2qiheWtQ3h"
   },
   "outputs": [],
   "source": [
    "def IOU(bb, bb_gt):\n",
    "\t\"\"\"\n",
    "\tCalculates the intersection-over-union for two bounding boxes\n",
    "\t\"\"\"\n",
    "\tx1 = max(bb[0,1], bb_gt[0,1])\n",
    "\ty1 = max(bb[0,0], bb_gt[0,0])\n",
    "\tx2 = min(bb[1,1], bb_gt[1,1])\n",
    "\ty2 = min(bb[1,0], bb_gt[1,0])\n",
    "\n",
    "\tw = x2-x1+1\n",
    "\th = y2-y1+1\n",
    "\n",
    "\t# handle odd cases of no intersection\n",
    "\tif (w < 0 and h < 0):\n",
    "\t\treturn 0\n",
    "\n",
    "\tinter = w*h\n",
    "\t\n",
    "\taarea = (bb[1,1]-bb[0,1]+1) * (bb[1,0]-bb[0,0]+1)\n",
    "\t\n",
    "\tbarea = (bb_gt[1,1]-bb_gt[0,1]+1) * (bb_gt[1,0]-bb_gt[0,0]+1)\n",
    "\t# intersection over union overlap\n",
    "\tiou = np.float32(inter) / (aarea+barea-inter)\n",
    "\t# set invalid entries to 0 iou - occurs when there is no overlap in x and y\n",
    "\tif iou < 0 or iou > 1:\n",
    "\t\treturn 0\n",
    "\treturn iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP4zh0F50T4m"
   },
   "source": [
    "# Main Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL-cl7de0Vbp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "# import cPickle as pickle\n",
    "import pickle\n",
    "\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "# K.set_image_dim_ordering('tf')\n",
    "# import keras\n",
    "# keras.backend.image_data_format() == 'channels_last'\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "# Local helpers\n",
    "# import image_actions\n",
    "# import reinforcement_helper\n",
    "# import action_functions\n",
    "# import image_loader\n",
    "# import image_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvO2qnuWbi-8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zU4jVci0Y_G"
   },
   "outputs": [],
   "source": [
    "# Flag to use either VOC dataset or patch dataset stored as pickle\n",
    "# VOC = True\n",
    "\n",
    "# Paths\n",
    "# project_root = '/media/ersy/Other/Google Drive/QM Work/Queen Mary/Course/Final Project/'\n",
    "# VOC2007_path = project_root+ 'Reinforcement learning/VOCdevkit/VOC2007'\n",
    "# VOC2012_path = project_root+ 'Reinforcement learning/VOCdevkit/VOC2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOyKykU60bug"
   },
   "outputs": [],
   "source": [
    "# if VOC == True:\n",
    "# \t# desired_class_set = 'aeroplane_trainval'\n",
    "# \t# desired_class = 'person'\n",
    "#     desired_class_set = 'tomato'\n",
    "#     desired_class = 'tomato'\n",
    "\n",
    "# \t### loading up VOC2007 images of a given class\n",
    "# \timg_name_list_2007 = image_actions.get_img_names(VOC2007_path, desired_class_set)\n",
    "# \timg_list_2007 = image_actions.load_images(VOC2007_path, img_name_list_2007) \n",
    "# \timg_list_2007, groundtruths_2007, img_name_list_2007 = image_loader.get_class_images(VOC2007_path, desired_class, img_name_list_2007, img_list_2007)\n",
    "\n",
    "# \tdesired_class_set = 'person_train'\n",
    "\n",
    "# \t### loading up VOC2012 images of a given class\n",
    "# \timg_name_list_2012 = image_actions.get_img_names(VOC2012_path, desired_class_set)\n",
    "# \timg_list_2012 = image_actions.load_images(VOC2012_path, img_name_list_2012) \n",
    "# \timg_list_2012, groundtruths_2012, img_name_list_2012 = image_loader.get_class_images(VOC2012_path, desired_class, img_name_list_2012, img_list_2012)\n",
    "\n",
    "# \t### combine 2007 and 2012 datasets\n",
    "# \timg_list = img_list_2007+img_list_2012\n",
    "# \tgroundtruths = groundtruths_2007+groundtruths_2012\n",
    "# \timg_name_list = img_name_list_2007+img_name_list_2012\n",
    "\n",
    "# else:\n",
    "# \tpatches_file = 'Experiment_2_Train_images.pickle'\n",
    "# \tpatches_bb_file = 'Experiment_2_Train_boxes.pickle'\n",
    "# \timg_list = pickle.load(open(project_root+'project_code/pickled_data/'+patches_file, 'rb'))\n",
    "# \tgroundtruths = pickle.load(open(project_root+'project_code/pickled_data/'+patches_bb_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QtFM65-VhPP"
   },
   "source": [
    "# Custom Data input from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDqfzXWYV7Uq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import os.path\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHWDyzCkVki6"
   },
   "outputs": [],
   "source": [
    "data_dir = '/gdrive/MyDrive/AOBD/Pascal_tomato'\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'valid')\n",
    "test_dir = os.path.join(data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pk9mO75oVndx"
   },
   "outputs": [],
   "source": [
    "train_xml = glob.glob(f\"{train_dir}/*.xml\")\n",
    "train_images = glob.glob(f\"{train_dir}/*.jpg\")\n",
    "\n",
    "val_xml = glob.glob(f\"{val_dir}/*.xml\")\n",
    "val_images = glob.glob(f\"{val_dir}/*.jpg\")\n",
    "\n",
    "test_xml = glob.glob(f\"{test_dir}/*.xml\")\n",
    "test_images = glob.glob(f\"{test_dir}/*.jpg\")\n",
    "\n",
    "train_images = sorted(train_images)\n",
    "train_xml = sorted(train_xml)\n",
    "val_images = sorted(val_images)\n",
    "val_xml = sorted(val_xml)\n",
    "test_images = sorted(test_images)\n",
    "test_xml = sorted(test_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxSrDCqOXOGK"
   },
   "outputs": [],
   "source": [
    "img_name_list = list(map(lambda x: str(x)[:-4].split('/')[-1], train_images))\n",
    "#img_list = [image.load_img(i) for i in train_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaeNg9vHVnaJ"
   },
   "outputs": [],
   "source": [
    "# false = 0\n",
    "# assert len(train_xml) == len(train_images)\n",
    "# for i in range(len(train_xml)):\n",
    "#     if sorted(train_images)[i][:-4] != sorted(train_xml)[i][:-4]:\n",
    "#         false += 1\n",
    "# print(false / len(train_xml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRAB3Nx6c6RN"
   },
   "outputs": [],
   "source": [
    "def get_bb_gt2(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    tomatoes = 0\n",
    "    x_min = []\n",
    "    x_max = []\n",
    "    y_min = []\n",
    "    y_max = []\n",
    "    for child in root:\n",
    "        # print(f'this child is {child.tag}')\n",
    "        if child.tag == 'object':\n",
    "            # print('Obj found')\n",
    "            for child2 in child:\n",
    "                # print(f'child2 is {child2}')\n",
    "                if child2.tag == 'name':\n",
    "                    tomatoes += 1\n",
    "                elif child2.tag == 'bndbox':\n",
    "                    for child3 in child2:\n",
    "                        if child3.tag == 'xmin':\n",
    "                            x_min.append(child3.text)\n",
    "                        elif child3.tag == 'xmax':\n",
    "                            x_max.append(child3.text)\n",
    "                        if child3.tag == 'ymin':\n",
    "                            y_min.append(child3.text)\n",
    "                        elif child3.tag == 'ymax':\n",
    "                            y_max.append(child3.text)\n",
    "    bb_list = []\n",
    "    category = [0] * tomatoes\n",
    "\n",
    "    # print(x_max)\n",
    "    # print(tomatoes)\n",
    "    # print(category)\n",
    "\n",
    "    for i in range(tomatoes):\n",
    "        bb_list.append(np.array([[y_min[i], x_min[i]],[y_max[i], x_max[i]]]))\n",
    "    \n",
    "    return np.array(category, dtype='uint16'), np.array(bb_list, dtype='uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN5kPyAmoAD7"
   },
   "outputs": [],
   "source": [
    "def get_groundtruths(groundtruths, img_name_list, img_list):\n",
    "\n",
    "\tdesired_class_list_bb = []\n",
    "\tdesired_class_list_image = []\n",
    "\tdesired_class_list_name = []\n",
    "\n",
    "\t# collect bounding boxes for each image\n",
    "\tfor image_ix in range(len(groundtruths)):\n",
    "\t\tcurrent_image_groundtruth = []\n",
    "\t\tground_image_bb_gt = groundtruths[image_ix]\n",
    "\t\t\n",
    "\t\t# flag the image as containing the desired target object\n",
    "\t\timage_flag = False\t\n",
    "\t\tfor ix in range(len(ground_image_bb_gt[0])):\t\n",
    "\t\t\tif ground_image_bb_gt[0][ix] == 0:\n",
    "\t\t\t\tcurrent_image_groundtruth.append(ground_image_bb_gt[1][ix])\n",
    "\t\t\t\timage_flag = True\n",
    "\n",
    "\t\t# append images that contain desired object\n",
    "\t\tif image_flag:\n",
    "\t\t\tdesired_class_list_bb.append(current_image_groundtruth)\t\n",
    "\t\t\t# desired_class_list_image.append(img_list[image_ix])\n",
    "\t\t\t# desired_class_list_name.append(img_name_list[image_ix])\n",
    "\n",
    "\treturn desired_class_list_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKYdZjFmasLl"
   },
   "outputs": [],
   "source": [
    "#groundtruths = []\n",
    "\n",
    "#for img_path in train_xml:\n",
    "    #groundtruths.append(get_bb_gt2(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_UR97I1qAN9"
   },
   "outputs": [],
   "source": [
    "#groundtruths2 = get_groundtruths(groundtruths, img_name_list, img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_tuiVHeVlLe"
   },
   "source": [
    "# Main Loop continues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilXPz1VHYHq5"
   },
   "outputs": [],
   "source": [
    "project_root = '/gdrive/MyDrive/AOBD/Project#1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHzAnt9xQvW2"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(project_root,'data.pkl'), 'rb') as fh:\n",
    "    img_list, groundtruths2 = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-7c4dAo0hnx"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "number_of_actions = 5\n",
    "history_length = 8\n",
    "Q_net_input_size = (25128, )\n",
    "visual_descriptor_size = 25088\n",
    "\n",
    "\n",
    "# Models\n",
    "### VGG16 model without top\n",
    "vgg16_conv = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "# initialise Q network (randomly or with existing weights) \n",
    "#loaded_weights_name = 'combi_aeroplane_180717_02_appr_forcedIOU06_augoff.hdf5'\n",
    "#loaded_weights = project_root+'project_code/network_weights/'+loaded_weights_name\n",
    "# loaded_weights = '0'\n",
    "loaded_weights_fname = 'q_weights.hdf5'\n",
    "loaded_weights = os.path.join(project_root, loaded_weights_fname)\n",
    "\n",
    "Q_net = get_q_network(shape_of_input=Q_net_input_size, number_of_actions=number_of_actions, weights_path=loaded_weights)\n",
    "\n",
    "# Validation callback\n",
    "saved_weights = 'saved_weights.hdf5'\n",
    "filepath= os.path.join(project_root, saved_weights)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "Plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "callbacks_list = []#[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1cdP4S-0icI"
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "episodes = 1\n",
    "epsilon = 1.1\n",
    "epsilon_decay = 0.1\n",
    "gamma = 0.9\n",
    "T = 50\n",
    "force_terminal = 0.6 # IoU to force terminal action\n",
    "training_epochs = 30\n",
    "guided_learning = True # Flag for guided learning on exploration\n",
    "augmented = False\n",
    "logging = False\n",
    "\n",
    "# example/batch size handling (controls for RAM VRAM constraints)\n",
    "conv_predict_batch_size = 10 # Decrease value if low on VRAM\n",
    "Q_predict_batch_size = 100\n",
    "Q_train_batch_size = 100\n",
    "\n",
    "\n",
    "\n",
    "chunk_factor = int(len(img_list)/128) #10 # Increase value if low on RAM\n",
    "chunk_size = int(len(img_list)/chunk_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRK7sRzy0l0J"
   },
   "outputs": [],
   "source": [
    "from numpy import matlib\n",
    "\n",
    "# Metric collection during training process\n",
    "action_counts = []\n",
    "avg_reward = []\n",
    "std_reward = []\n",
    "\n",
    "\n",
    "episode_time = time.time()\n",
    "\n",
    "for episode in range(episodes):\n",
    "\tprint(\"Episode:\", episode)\n",
    "\n",
    "\tprint(\"Time taken = \", time.time() - episode_time)\n",
    "\tepisode_time = time.time()\n",
    "\n",
    "\t# change the exploration-eploitation tradeoff as the episode count increases\n",
    "\tif epsilon > 0.11:\n",
    "\t\tepsilon = epsilon - epsilon_decay\n",
    "\n",
    "\t# initialise collections for per episode metrics\n",
    "\taction_count = [0,0,0,0,0]\n",
    "\tepisode_rewards = []\n",
    "\n",
    "\tfor chunk in range(chunk_factor):\n",
    "\t\t# list to store experiences, new one for each episode\n",
    "\t\texperiences = []\n",
    "\n",
    "\t\t# determines the offset to use when iterating through the chunk\n",
    "\t\tchunk_offset = chunk*chunk_size\n",
    "\n",
    "\t\t# iteration through all images in the current chunk\n",
    "\t\tfor image_ix in range(chunk_offset,chunk_offset + chunk_size):\n",
    "\t\t\tprint(\"Image:\", image_ix)\n",
    "\n",
    "\t\t\t# get initial parameters for each image\n",
    "\t\t\toriginal_image = np.array(img_list[image_ix])\n",
    "\t\t\timage = np.array(img_list[image_ix])\n",
    "\t\t\timage_dimensions = image.shape[:-1]\n",
    "\n",
    "\t\t\t# collect bounding boxes for each image\n",
    "\t\t\tground_image_bb_gt = groundtruths2[image_ix]#image_actions.get_bb_gt(image_name)\n",
    "\n",
    "\t\t\t# data augmentation -> 0.5 probability of flipping image and bounding box horizontally\n",
    "\t\t\t# if augmented:\n",
    "\t\t\t# \taugment = bool(random.getrandbits(1))\n",
    "\t\t\t# \tif augment:\n",
    "\t\t\t# \t\toriginal_image, ground_image_bb_gt = image_augmentation.flip_image(original_image, ground_image_bb_gt)\n",
    "\t\t\t# \t\timage = np.fliplr(image)\n",
    "\n",
    "\t\t\t# initial bounding box (whole image, raw size)\n",
    "\t\t\tboundingbox = np.array([[0,0],image_dimensions])\n",
    "\n",
    "\t\t\t# list to store IOU for each object in the image and current bounding box\n",
    "\t\t\tIOU_list = []\n",
    "\n",
    "\t\t\timage_IOU = []\n",
    "\t\t\t# get the initial IOU for each object\n",
    "\t\t\tfor ground_truth in ground_image_bb_gt:\n",
    "\t\t\t\tcurrent_iou = IOU(ground_truth, boundingbox)\n",
    "\t\t\t\timage_IOU.append(current_iou)\n",
    "\t\t\tIOU_list.append(image_IOU)\n",
    "\n",
    "\t\t\t# create the history vector\n",
    "\t\t\thistory_vec = np.zeros((number_of_actions, history_length))\n",
    "\n",
    "\t\t\t# preprocess the image\n",
    "\t\t\tpreprocessed_image = image_preprocessing(original_image)\n",
    "\n",
    "\t\t\t# intiialise experience subcontainer for each image\n",
    "\t\t\texperiences.append([])\n",
    "\n",
    "\t\t\t# collecting the preprocessed images in a separate list, the history, and an index of states already calculated\n",
    "\t\t\tpreprocessed_list = []\n",
    "\t\t\thistory_list = []\n",
    "\t\t\texploitation_index = []\n",
    "\t\t\texploitation_states = []\n",
    "\t\t\timage_rewards = []\n",
    "\n",
    "\n",
    "\t\t\tfor t in range(T):\n",
    "\t\t\t\t# collect the preprocessed image\n",
    "\t\t\t\tpreprocessed_list.append(preprocessed_image)\n",
    "\t\t\t\thistory_list.append(np.array(np.reshape(history_vec, (number_of_actions*history_length))))\n",
    "\n",
    "\t\t\t\t# add action history to experience collection\n",
    "\t\t\t\texperiences[image_ix-chunk_offset].append([np.array(np.reshape(history_vec, (number_of_actions*history_length)))])\n",
    "\n",
    "\t\t\t\t# exploration or exploitation\n",
    "\t\t\t\tif random.uniform(0,1) < epsilon:\n",
    "\t\t   \n",
    "\t\t   \t\t\t# limit exploration actions to only positive actions\n",
    "\t\t\t\t\tif guided_learning:\n",
    "\n",
    "\t\t\t\t\t\t# collect positive actions\n",
    "\t\t\t\t\t\tgood_actions = []\n",
    "\t\t\t\t\t\tfor act in range(number_of_actions-1):\n",
    "\t\t\t\t\t\t\tpotential_image, potential_boundingbox = crop_image(original_image, boundingbox, act)            \n",
    "\t\t\t\t\t\t\tpotential_image_IOU = []\n",
    "\n",
    "\t\t\t\t\t\t\t# check for IoU change for each action\n",
    "\t\t\t\t\t\t\tfor ground_truth in ground_image_bb_gt:\n",
    "\t\t\t\t\t\t\t\tpotential_iou = IOU(ground_truth, potential_boundingbox)\n",
    "\t\t\t\t\t\t\t\tpotential_image_IOU.append(potential_iou)\n",
    "\n",
    "\t\t\t\t\t\t\t# store only positive actions\n",
    "\t\t\t\t\t\t\tif max(potential_image_IOU) >= max(image_IOU):\n",
    "\t\t\t\t\t\t\t\tgood_actions.append(act)\n",
    "\n",
    "\t\t\t\t\t\t# make a selection out of the positive actions of possible\n",
    "\t\t\t\t\t\tif len(good_actions) > 0:\n",
    "\t\t\t\t\t\t\tgood_actions.append(number_of_actions-1)\n",
    "\t\t\t\t\t\t\taction = random.choice(good_actions)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\taction = random.randint(0, number_of_actions-1)\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\taction = random.randint(0, number_of_actions-1)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t# if the IOU is greater than 0.5 force the action to be the terminal action\n",
    "\t\t\t\t# this is done to help speed up the training process\n",
    "\t\t\t\telif max(image_IOU) > force_terminal:\n",
    "\t\t\t\t\taction = number_of_actions-1\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Exploitation\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tstate_vec = get_state_as_vec(preprocessed_image, history_vec, vgg16_conv) ### ADDED!!!\n",
    "\t\t\t\t\tQ_vals = Q_net.predict(state_vec)\n",
    "\t\t\t\t\taction = np.argmax(Q_vals)\n",
    "\n",
    "\t\t\t\t\t# collect the time step value for states that have already been calculated\n",
    "\t\t\t\t\texploitation_states.append(state_vec)\n",
    "\t\t\t\t\texploitation_index.append(t)\n",
    "\n",
    "\n",
    "\t\t\t\t# if in training the termination action is used no need to get the subcrop again\n",
    "\t\t\t\tif action != number_of_actions-1:\n",
    "\t\t\t\t\timage, boundingbox = crop_image(original_image, boundingbox, action)\n",
    "\n",
    "\n",
    "\t\t\t\t# measure IOU\n",
    "\t\t\t\timage_IOU = []\n",
    "\t\t\t\tfor ground_truth in ground_image_bb_gt:\n",
    "\t\t\t\t\tcurrent_iou = IOU(ground_truth, boundingbox)\n",
    "\t\t\t\t\timage_IOU.append(current_iou)\n",
    "\t\t\t\tIOU_list.append(image_IOU)\n",
    "\n",
    "\t\t\t\t# get reward if termination action is taken\n",
    "\t\t\t\treward = get_reward(action, IOU_list, t)\n",
    "\n",
    "\t\t\t\t# update history vector\n",
    "\t\t\t\thistory_vec[:, :-1] = history_vec[:,1:]\n",
    "\t\t\t\thistory_vec[:,-1] = [0,0,0,0,0] # hard coded actions here\n",
    "\t\t\t\thistory_vec[action, -1] = 1\n",
    "\n",
    "\t\t\t\tpreprocessed_image = image_preprocessing(image)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# add action, reward, history to experience list\n",
    "\t\t\t\texperiences[image_ix-chunk_offset][t].append(action)\n",
    "\t\t\t\texperiences[image_ix-chunk_offset][t].append(reward)\n",
    "\t\t\t\texperiences[image_ix-chunk_offset][t].append(np.array(np.reshape(history_vec, (number_of_actions*history_length)))) # ADDED!!!\n",
    "\n",
    "\t\t\t\t# collect episode metrics\n",
    "\t\t\t\taction_count[action] += 1\n",
    "\t\t\t\timage_rewards.append(reward)\n",
    "\n",
    "\t\t\tepisode_rewards.append(sum(image_rewards))\n",
    "\n",
    "\t\t\t### CONVERTING COLLECTED IMAGES TO CONV OUTPUTS\n",
    "\t\t\t# collect the last preprocessed image for this given image\n",
    "\t\t\tpreprocessed_list.append(preprocessed_image)\n",
    "\t\t\t\n",
    "\t\t\t# collecting the final history state\n",
    "\t\t\tfinal_history = np.array(np.reshape(history_vec, (number_of_actions*history_length)))\n",
    "\t\t\thistory_list.append(final_history)\n",
    "\t\t\thistory_arr = np.vstack(history_list)\n",
    "\n",
    "\t\t\t# get the indexes that correspond to the conv_outputs\n",
    "\t\t\ttodo_states = [i for i in range(T+1) if i not in exploitation_index]\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# preprocessed image -> conv output for a single image\n",
    "\t\t\tconv_output = np.array(preprocessed_list).squeeze(1)\n",
    "\t\t\tconv_output = vgg16_conv.predict(conv_output[todo_states], conv_predict_batch_size, verbose=1)\n",
    "\t\t\tconv_output = np.reshape(conv_output, (conv_output.shape[0], visual_descriptor_size))\n",
    "\n",
    "\t\t\t# get the precalculated states if any\n",
    "\t\t\ttry:\n",
    "\t\t\t\texploitation_states = np.vstack(exploitation_states)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\n",
    "\t\t\t# add the history to the conv_output, combine with exploitation states (if any) and reorder by timestep\n",
    "\t\t\tconv_states = np.append(conv_output, history_arr[todo_states], axis=1)\n",
    "\t\t\ttry:\n",
    "\t\t\t\tconv_states = np.append(conv_states, exploitation_states, axis=0)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\n",
    "\t\t\t# add the exploited indexes and sort conv_states back into the correct order\n",
    "\t\t\ttodo_states.extend(exploitation_index)\n",
    "\t\t\tconv_states = [x for (y, x) in sorted(zip(todo_states, conv_states))]\n",
    "\n",
    "\t\t\t[experiences[image_ix-chunk_offset][i].append(conv_states[i]) for i in range(T)]\n",
    "\t\t\t[experiences[image_ix-chunk_offset][i].append(conv_states[i+1]) for i in range(T)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Actual training per given episode over a set number of experiences (training iterations)\n",
    "\t\t# flatten the experiences list for learning\n",
    "\t\tflat_experiences = [x for l in experiences for x in l]\n",
    "\t\tnum_of_experiences = len(flat_experiences) \n",
    "\t\t\n",
    "\t\trandom_experiences = np.array(flat_experiences)\n",
    "\n",
    "\t\t# delete variables to free up memory\n",
    "\t\tdel flat_experiences\n",
    "\n",
    "\t\tinitial_state = np.array([state[4] for state in random_experiences]) \n",
    "\t\tnext_state = np.array([state[5] for state in random_experiences])\n",
    "\n",
    "\n",
    "\t\t# calculating the Q values for the initial state\n",
    "\t\tinitial_Q = Q_net.predict(initial_state, Q_predict_batch_size, verbose=1)\n",
    "\n",
    "\t\t# calculating the Q values for the next state\n",
    "\t\tnext_Q = Q_net.predict(next_state, Q_predict_batch_size, verbose=1)\n",
    "\t\t\n",
    "\t\t# calculating the maximum Q for the next state\n",
    "\t\tnext_Q_max = next_Q.max(axis=1)\n",
    "\n",
    "\t\t# get the reward for a given experience\n",
    "\t\t# random_reward = np.expand_dims(random_experiences[:, 2], 1)\n",
    "\t\trandom_reward = random_experiences[:, 2]\n",
    "\n",
    "\t\t# get the action of a given experience\n",
    "\t\trandom_actions = np.expand_dims(random_experiences[:, 1], 1)\n",
    "\t\tflat_actions = [x for l in random_actions for x in l]\n",
    "\n",
    "\t\t# collect the indexes of terminal actions and set next state Q value to 0\n",
    "\t\t# if the terminal action is selected the episode ends and there should be no additional reward\n",
    "\t\tterminal_indices = [i for i, x in enumerate(flat_actions) if x == number_of_actions-1]\n",
    "\t\tnext_Q_max[terminal_indices] = 0\n",
    "\n",
    "\t\t# discount the future reward, i.e the Q value output\n",
    "\t\ttarget = np.array(next_Q_max) * gamma\n",
    "\n",
    "\t\t# target for the current state should be the Q value of the next state - the reward \n",
    "\t\ttarget = target + random_reward\n",
    "\n",
    "\t\t# repeat the target array to the same size as the initial_Q array (allowing the cost to be limited to the selected actions)\n",
    "\t\ttarget_repeated = matlib.repmat(target, 5, 1).T\n",
    "\n",
    "\t\t# this takes the initial Q values for the state and replaces only the Q values for the actions that were used to the new target, else the error should be 0\n",
    "\t\tinitial_Q[np.arange(len(initial_Q)), flat_actions] = target_repeated[np.arange(len(target_repeated)), flat_actions]\n",
    "\n",
    "\t\t# nicer names\n",
    "\t\ttraining_input = initial_state\n",
    "\t\ttraining_target = initial_Q\n",
    "\n",
    "\n",
    "\t\tbefore = time.time()\n",
    "\t\tQ_net.fit(training_input, training_target, epochs=training_epochs, batch_size=Q_train_batch_size, shuffle=True, verbose=1)#, callbacks=callbacks_list, validation_split=0.2)\n",
    "\t\tafter = time.time()\n",
    "\t\tprint(\"Time taken =\", after-before)\n",
    "\t\tprint(\"Saving weights...\")\n",
    "\t\tQ_net.save_weights(os.path.join(project_root, 'q_weights.hdf5'))\n",
    "\t\t\n",
    "        # delete variables to free up memory\n",
    "\t\tdel initial_state\n",
    "\t\tdel next_state\n",
    "\t\tdel random_experiences\n",
    "\n",
    "\n",
    "\t# collect the counts of actions taken per episode\n",
    "\taction_counts.append(action_count)\n",
    "\tavg_reward.append(np.mean(episode_rewards))\n",
    "\tstd_reward.append(np.std(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5teYauWHMx5"
   },
   "outputs": [],
   "source": [
    "# t1 = cv2.imread('https://firebasestorage.googleapis.com/v0/b/tomatodetection-47395.appspot.com/o/images%2Ffoo.jpg?alt=media&token=f55832ff-3ece-4a86-83b3-051978eb6c0a')\n",
    "# cv2_imshow(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40R4b5Tj0q_V"
   },
   "outputs": [],
   "source": [
    "# Log of training parameters\n",
    "logging = True\n",
    "if logging:\n",
    "\tlog_location = os.path.join(project_root, 'logs')\n",
    "\n",
    "\tlog_names = ['loaded_weights','episodes', 'epsilon','gamma', \n",
    "\t\t\t\t\t'Time_steps', 'movement_reward', 'terminal_reward_5', 'terminal_reward_7', 'terminal_reward_9',\n",
    "\t\t\t\t\t'iou_threshold_5', 'iou_threshold_7','iou_threshold_9','update_step', 'force_terminal']\n",
    "\n",
    "\tlog_vars = [loaded_weights, episodes, epsilon, gamma, T,movement_reward,\n",
    "\t\t\t\tterminal_reward_5,terminal_reward_7,terminal_reward_9,\n",
    "\t\t\t\tiou_threshold_5, iou_threshold_7,iou_threshold_9,\n",
    "\t\t\t\tupdate_step, force_terminal]\n",
    "\n",
    "\twith open(os.path.join(log_location,'saved_weights.csv'), 'w') as csvfile:\n",
    "\t\tdetails = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\t\tdetails.writerow(log_names)\t\n",
    "\t\tdetails.writerow(log_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Szx9jxXS0tZZ"
   },
   "outputs": [],
   "source": [
    "# plotting average reward per action over each episode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "minus_std = [avg_reward[i] - std_reward[i] for i in range(len(avg_reward))]\n",
    "plus_std = [avg_reward[i] + std_reward[i] for i in range(len(avg_reward))]\n",
    "plt.plot(avg_reward, label='Average Reward', color='black')\n",
    "plt.plot(minus_std, label='-1 St. Dev', linestyle='--', color='red')\n",
    "plt.plot(plus_std, label='+1 St. Dev', linestyle='--', color='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward per Image')\n",
    "plt.title('Changes in Average Reward for each Image through the Learning Process')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEe4Oah3tYtF"
   },
   "source": [
    "# Single Image Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi7IC9HmtcZv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhXzKZgNwPYR"
   },
   "outputs": [],
   "source": [
    "val_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3ZOx1tL7B7t"
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QrJ5FxOtlTU"
   },
   "outputs": [],
   "source": [
    "### Vars\n",
    "# project_root = '/media/ersy/Other/Google Drive/QM Work/Queen Mary/Course/Final Project/'\n",
    "# VOC_path = project_root+ 'Reinforcement learning/VOCdevkit/VOC2007'\n",
    "\n",
    "# image_path = \"/home/ersy/Downloads/person_example.jpg\"\n",
    "image_path = train_images[0]\n",
    "loaded_image = image.load_img(image_path, False)\n",
    "\n",
    "\n",
    "\n",
    "number_of_actions = 5\n",
    "history_length = 8\n",
    "Q_net_input_size = (25128, )\n",
    "\n",
    "\n",
    "### VGG16 model without top\n",
    "vgg16_conv = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "saved_weights = 'q_weights.hdf5'\n",
    "weights = os.path.join(project_root, saved_weights)\n",
    "\n",
    "Q_net = get_q_network(shape_of_input=Q_net_input_size, number_of_actions=number_of_actions, weights_path=weights)\n",
    "\n",
    "### Q network definition\n",
    "epsilon = 0\n",
    "T = 60\n",
    "\n",
    "\n",
    "# convert image to array\t\n",
    "original_image = np.array(loaded_image)\n",
    "image_copy = np.copy(original_image)\n",
    "image_dimensions = image_copy.shape[:-1]\n",
    "\n",
    "# create the history vector\n",
    "history_vec = np.zeros((number_of_actions, history_length))\n",
    "\n",
    "# preprocess the image\n",
    "preprocessed_image = image_preprocessing(original_image)\n",
    "\n",
    "# get initial state vector\n",
    "state_vec = get_state_as_vec(preprocessed_image, history_vec, vgg16_conv)\n",
    "\n",
    "# get initial bounding box\n",
    "boundingbox = np.array([[0,0],image_dimensions])\n",
    "\n",
    "all_proposals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W64EM2hgttIY"
   },
   "outputs": [],
   "source": [
    "for t in range(T):\n",
    "\t\tprint('Time Step: ', t)\n",
    "\t\t# add the current state to the experience list\n",
    "\t\tall_proposals.append(boundingbox)\n",
    "\n",
    "\t\t# plug state into Q network\n",
    "\t\tQ_vals = Q_net.predict(state_vec)\n",
    "\n",
    "\t\taction = np.argmax(Q_vals)\n",
    "\n",
    "\n",
    "\t\tif action != number_of_actions-1:\n",
    "\t\t\timage_copy, boundingbox = crop_image(original_image, boundingbox, action)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"This is your object!\")\n",
    "\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\t# update history vector\n",
    "\t\thistory_vec[:, :-1] = history_vec[:,1:]\n",
    "\t\thistory_vec[:,-1] = [0,0,0,0,0] # hard coded actions here\n",
    "\t\thistory_vec[action, -1] = 1\n",
    "\n",
    "\t\tpreprocessed_image = image_preprocessing(image_copy)\n",
    "\t\tstate_vec = get_state_as_vec(preprocessed_image, history_vec, vgg16_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DU_hJ85Otxfo"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(original_image)\n",
    "\n",
    "num_of_proposals = len(all_proposals)\n",
    "color = plt.cm.rainbow(np.linspace(0,1,num_of_proposals))\n",
    "\n",
    "for proposal, c in zip(all_proposals, color):\n",
    "    top_left = (proposal[0,1], proposal[0,0])\n",
    "    width = proposal[1,1] - proposal[0,1]\n",
    "    height = proposal[1,0] - proposal[0,0]\n",
    "    rect = patches.Rectangle(top_left, width, height, linewidth=2, edgecolor=c, facecolor='none') # change facecolor to add fill\n",
    "    ax.add_patch(rect)\n",
    "rect = patches.Rectangle(top_left, width, height, linewidth=2, edgecolor='white', facecolor='none' , label='proposal')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbaDy3xltzTh"
   },
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwhS-vRdt1Ou"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib\n",
    "#matplotlib.use(\"webagg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import collections\n",
    "import cPickle as pickle\n",
    "\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "\n",
    "### Local helpers\n",
    "import image_actions\n",
    "import reinforcement_helper\n",
    "import action_functions\n",
    "import image_loader\n",
    "\n",
    "### \n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRgwIbKht_YE"
   },
   "outputs": [],
   "source": [
    "### Vars\n",
    "project_root = '/media/ersy/Other/Google Drive/QM Work/Queen Mary/Course/Final Project/'\n",
    "VOC_path = project_root+ 'Reinforcement learning/VOCdevkit/VOC2007'\n",
    "\n",
    "# parser for the input, defining the number of training epochs and an image\n",
    "parser = argparse.ArgumentParser(description = 'Epoch: ')\n",
    "parser.add_argument('-n', metavar='N', type=int, default=0)\n",
    "parser.add_argument(\"-i\", \"--image\", help=\"path to the input image\")\n",
    "args = vars(parser.parse_args())\n",
    "epochs_id = args['n']\n",
    "image = args['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vL8zDP9iuCnf"
   },
   "outputs": [],
   "source": [
    "VOC = True\n",
    "if VOC:\n",
    "\t### loading up VOC images of a given class\n",
    "\tclass_file = 'person_test'\n",
    "\timg_name_list = image_actions.get_img_names(VOC_path, class_file)\n",
    "\timg_list = image_actions.load_images(VOC_path, img_name_list) \n",
    "\n",
    "\tdesired_class = 'person'\n",
    "\n",
    "\timg_list, groundtruths, img_name_list = image_loader.get_class_images(VOC_path, desired_class, img_name_list, img_list)\n",
    "else:\n",
    "\tclass_file = 'Experiment_1'\n",
    "\timg_list = pickle.load(open(project_root+'project_code/pickled_data/Experiment_8_Test_images.pickle', 'rb'))\n",
    "\tgroundtruths = pickle.load(open(project_root+'project_code/pickled_data/Experiment_8_Test_boxes.pickle', 'rb'))\n",
    "\n",
    "# DEBUG: Overfitting hack\n",
    "#img_list = img_list[0:8]\n",
    "#groundtruths = groundtruths[0:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbMrfmxct5-P"
   },
   "outputs": [],
   "source": [
    "number_of_actions = 5\n",
    "history_length = 8\n",
    "Q_net_input_size = (25128, )\n",
    "\n",
    "\n",
    "### VGG16 model without top\n",
    "vgg16_conv = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "# path for non validated set\n",
    "#weights_path = '/media/ersy/Other/Google Drive/QM Work/Queen Mary/Course/Final Project/project_code/network_weights/no_validation/'\n",
    "\n",
    "weights_path = '/media/ersy/Other/Google Drive/QM Work/Queen Mary/Course/Final Project/project_code/network_weights/final_weights/'\n",
    "\n",
    "# change the weights loaded for Q network testing\n",
    "saved_weights = 'Person_TEST.hdf5'\n",
    "weights = weights_path+saved_weights\n",
    "\n",
    "Q_net = reinforcement_helper.get_q_network(shape_of_input=Q_net_input_size, number_of_actions=number_of_actions, weights_path=weights)\n",
    "\n",
    "### Q network definition\n",
    "epsilon = 0\n",
    "T = 60\n",
    "# stores proposal regions\n",
    "all_proposals = []\n",
    "\n",
    "# stores ground truth regions\n",
    "all_ground_truth = []\n",
    "\n",
    "all_IOU = []\n",
    "\n",
    "all_actions = []\n",
    "\n",
    "all_image_scale= []\n",
    "all_image_centre = []\n",
    "\n",
    "# IOU for terminal actions - for use in calulating evaluation stats\n",
    "terminal_IOU = []\n",
    "terminal_index = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-hAStBRuJJa"
   },
   "outputs": [],
   "source": [
    "# loop through images\n",
    "for image_ix in range(len(img_list)):\n",
    "\t\n",
    "\toriginal_image = np.array(img_list[image_ix])\n",
    "\n",
    "\tprint(\"new image: \", image_ix)\n",
    "\t# get initial parameters for each image\n",
    "\n",
    "\timage = np.copy(original_image)\n",
    "\t#image_name = img_name_list[image_ix]\n",
    "\timage_dimensions = image.shape[:-1]\n",
    "\n",
    "\t# collect bounding boxes for each image\n",
    "\tground_image_bb_gt = groundtruths[image_ix]\n",
    "\n",
    "\t# METRICS: get the scale of the object relative to the image size\n",
    "\t\n",
    "\timage_scale = []\n",
    "\timage_centre = []\n",
    "\tfor box in ground_image_bb_gt:\n",
    "\n",
    "\t\twidth = box[1][1] - box[0][1]\n",
    "\t\theight = box[1][0] - box[0][0]\n",
    "\t\tarea = width*height\n",
    "\n",
    "\t\timage_area = image_dimensions[0]*image_dimensions[1]\n",
    "\t\timage_scale.append(float(area)/image_area)\n",
    "\t\timage_centre.append([(box[1][0] + box[0][0])/2, (box[1][1] + box[0][1])/2])\n",
    "\tall_image_scale.append(image_scale)\n",
    "\tall_image_centre.append(image_centre)\n",
    "\n",
    "\t# add current image ground truth to all ground truths\n",
    "\tall_ground_truth.append(ground_image_bb_gt)\n",
    "\n",
    "\t# collect proposal bounding boxes\n",
    "\tboundingboxes = []\n",
    "\n",
    "\t#add image proposals to list of all proposals\n",
    "\tall_proposals.append(boundingboxes)\n",
    "\n",
    "\t# initial bounding box (whole image, raw size)\n",
    "\tboundingbox = np.array([[0,0],image_dimensions])\n",
    "\n",
    "\t# list to store IOU for each object in the image and current bounding box\n",
    "\tIOU_list = []\n",
    "\n",
    "\t# list to store actions taken for each image to associate with IOUs\n",
    "\t# the first IOU is associated with no action\n",
    "\taction_list = []\n",
    "\t\n",
    "\timage_IOU = []\n",
    "\t# get the IOU for each object\n",
    "\tfor ground_truth in ground_image_bb_gt:\n",
    "\t\tcurrent_iou = reinforcement_helper.IOU(ground_truth, boundingbox)\n",
    "\t\timage_IOU.append(current_iou)\n",
    "\tIOU_list.append(image_IOU)\n",
    "\n",
    "\t# create the history vector\n",
    "\thistory_vec = np.zeros((number_of_actions, history_length))\n",
    "\n",
    "\t# preprocess the image\n",
    "\tpreprocessed_image = image_actions.image_preprocessing(original_image)\n",
    "\n",
    "\t# get the state vector (conv output of VGG16 concatenated with the action history)\n",
    "\tstate_vec = reinforcement_helper.get_state_as_vec(preprocessed_image, history_vec, vgg16_conv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tfor t in range(T):\n",
    "\n",
    "\t\t# add the current state to the experience list\n",
    "\t\tall_proposals[image_ix].append(boundingbox)\n",
    "\n",
    "\t\t# plug state into Q network\n",
    "\t\tQ_vals = Q_net.predict(state_vec)\n",
    "\n",
    "\t\tbest_action = np.argmax(Q_vals)\n",
    "\n",
    "\t   # exploration or exploitation\n",
    "\t\tif random.uniform(0,1) < epsilon:\n",
    "\t\t\taction = random.randint(0, number_of_actions-1)\n",
    "\t\telse:\n",
    "\t\t\taction = best_action\n",
    "\n",
    "\t\tprint('action:', action)\n",
    "\n",
    "\t\tif action != number_of_actions-1:\n",
    "\t\t\timage, boundingbox = action_functions.crop_image(original_image, boundingbox, action)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"This is your object!\")\n",
    "\n",
    "\n",
    "\t\t\tcurrent_image_IOU = []\n",
    "\t\t\tfor ground_truth in ground_image_bb_gt:\n",
    "\t\t\t\tcurrent_iou = reinforcement_helper.IOU(ground_truth, boundingbox)\n",
    "\t\t\t\tcurrent_image_IOU.append(current_iou)\n",
    "\t\t\tprint(\"IOU: \", max(current_image_IOU))\n",
    "\n",
    "\t\t\tterminal_IOU.append(max(current_image_IOU))\n",
    "\t\t\tterminal_index.append(image_ix)\n",
    "\t\t\taction_list.append(action)\n",
    "\t\t\t#all_actions.append(action_list)\n",
    "\n",
    "\t\t\t# implement something to mask the region covered by the boundingbox\n",
    "\t\t\t# rerun for the image \n",
    "\t\t\t#mask =  [103.939, 116.779, 123.68]\n",
    "\t\t\t#original_image[boundingbox[0,0]:boundingbox[1,0], boundingbox[0,1]:boundingbox[1,1]] = mask\n",
    "\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\t# measure IOU\n",
    "\t\timage_IOU = []\n",
    "\t\tfor ground_truth in ground_image_bb_gt:\n",
    "\t\t\tcurrent_iou = reinforcement_helper.IOU(ground_truth, boundingbox)\n",
    "\t\t\timage_IOU.append(current_iou)\n",
    "\t\tIOU_list.append(image_IOU)\n",
    "\n",
    "\t\taction_list.append(action)\n",
    "\n",
    "\t\t# update history vector\n",
    "\t\thistory_vec[:, :-1] = history_vec[:,1:]\n",
    "\t\thistory_vec[:,-1] = [0,0,0,0,0] # hard coded actions here\n",
    "\t\thistory_vec[action, -1] = 1\n",
    "\n",
    "\t\tpreprocessed_image = image_actions.image_preprocessing(image)\n",
    "\t\tstate_vec = reinforcement_helper.get_state_as_vec(preprocessed_image, history_vec, vgg16_conv)\n",
    "\n",
    "\t# add the IOU calculated for each proposal for each image for evaluation purposes\n",
    "\tall_IOU.append(IOU_list)\n",
    "\tall_actions.append(action_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erXlikC1uNHV"
   },
   "outputs": [],
   "source": [
    "### EVALUATION AND METRICS\n",
    "\n",
    "# lets the proposals and ground truth bounding boxes be visualised\n",
    "ix = 0\n",
    "image_actions.view_results(img_list, all_ground_truth, all_proposals, all_IOU, ix)\n",
    "\n",
    "\n",
    "# simple evaluation metric\n",
    "detected = sum([i>=0.5 for i in terminal_IOU])\n",
    "termination_total = float(len(terminal_IOU))\n",
    "termination_accuracy = detected/termination_total\n",
    "print(\"termination accuracy = \", termination_accuracy)\n",
    "\n",
    "flat_objects = [x for l in groundtruths for x in l]\n",
    "total_objects = float(len(flat_objects))\n",
    "total_accuracy = detected/total_objects\n",
    "print('total accuracy = ', total_accuracy)\n",
    "\n",
    "# obtain the accuracy for the final proposal bounding box (regardless of whether the terminal action is triggered)\n",
    "final_proposal_IOU = [max(i[-1]) for i in all_IOU]\n",
    "final_proposal_detected = sum([i>0.5 for i in final_proposal_IOU])\n",
    "final_proposal_accuracy = final_proposal_detected/total_objects\n",
    "print('final proposal accuracy = ', final_proposal_accuracy)\n",
    "\n",
    "\n",
    "# turn list of IOUs for each image into separate object IOUs\n",
    "t1 = [[list(j) for j in zip(*i)] for i in all_IOU]\n",
    "t2 = [i for j in t1 for i in j]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(4, 1, sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwrQ5TpAuSzH"
   },
   "outputs": [],
   "source": [
    "# code for investigating actions taken for different images - assessing the agent performance\n",
    "\n",
    "# objects with the final IoU above 0.5 (terminal action called)\n",
    "IOU_above_cutoff  = [i for i in t2 if i[-1]>=0.5]\n",
    "\n",
    "# object \n",
    "IOU_below_cutoff = [i for i in t2 if i[-1]<0.5 and len(i) < T+1]\n",
    "\n",
    "# objects with no terminal action called\n",
    "IOU_no_terminal = [i for i in t2 if i[-1]<0.5 and len(i) == T+1]\n",
    "\n",
    "for img in IOU_above_cutoff:\n",
    "\tax[0].plot(img)\n",
    "ax[0].set_ylabel('IOU')\n",
    "ax[0].set_title('IOU above cutoff')\n",
    "ax[0].set_ylim(0,1)\n",
    "\n",
    "for img in IOU_below_cutoff:\n",
    "\tax[1].plot(img)\n",
    "ax[1].set_ylabel('IOU')\n",
    "ax[1].set_title('IOU below cutoff')\n",
    "ax[1].set_ylim(0,1)\n",
    "\n",
    "for img in IOU_no_terminal:\n",
    "\tax[2].plot(img)\n",
    "ax[2].set_ylabel('IOU')\n",
    "ax[2].set_title('IOU no terminal actions')\n",
    "ax[2].set_ylim(0,1)\n",
    "\n",
    "# storing the number of actions taken before the terminal action\n",
    "action_count = [len(i) for i in all_actions if i[-1] == 4]\n",
    "action_count_mean = sum(action_count)/len(action_count)\n",
    "counter = collections.Counter(action_count)\n",
    "\n",
    "ax[3].bar(counter.keys(), counter.values())\n",
    "ax[3].set_xlabel(\"Actions taken\")\n",
    "ax[3].set_ylabel(\"Count\")\n",
    "ax[3].set_title('Actions per image (terminal action used)')\n",
    "ax[3].axvline(action_count_mean, color='red', linewidth=2, label='MEAN: '+str(action_count_mean)[:5])\n",
    "ax[3].legend()\n",
    "\n",
    "plt.xlim(0,T)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "To3kL15Uuaak"
   },
   "outputs": [],
   "source": [
    "# calculating mAP\n",
    "# true positive -> IOU over 0.5 + terminal action\n",
    "# false positive -> IOU under 0.5 + terminal action\n",
    "# false negative -> no terminal action taken when image contains an object\n",
    "# true negative -> no terminal action taken when image does not contain an object\n",
    "\n",
    "TP = sum([i>=0.5 for i in terminal_IOU])\n",
    "FP = sum([i<0.5 for i in terminal_IOU])\n",
    "FN = total_objects-(TP+FP)\n",
    "\n",
    "AP = float(TP)/(TP+FP)\n",
    "\n",
    "if TP+FN > 0:\n",
    "\tRecall = float(TP)/(TP+FN)\n",
    "else:\n",
    "\tRecall = 0\n",
    "\n",
    "if AP > 0:\n",
    "\tF1 = AP*Recall/(AP+Recall)*2\n",
    "else:\n",
    "\tF1 = 0\n",
    "\n",
    "\n",
    "print('precision = ', AP)\n",
    "print('recall = ', Recall)\n",
    "print('F1 = ', F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IILp4QuKudiv"
   },
   "outputs": [],
   "source": [
    "average_terminal_IOU = sum(terminal_IOU)/len(terminal_IOU)\n",
    "print(\"average terminal IOU = \", average_terminal_IOU)\n",
    "std_terminal_IOU = np.std(terminal_IOU)\n",
    "print(\"terminal IOU standard deviation = \", std_terminal_IOU)\n",
    "average_TP_IOU = sum([i for i in terminal_IOU if i>=0.5])/TP if TP >0 else np.nan\n",
    "print(\"average TP IOU = \", average_TP_IOU)\n",
    "average_FP_IOU = sum([i for i in terminal_IOU if i<0.5])/FP if FP>0 else np.nan\n",
    "print(\"average FP IOU = \", average_FP_IOU)\n",
    "\n",
    "# Plot distributions of terminal IOUs\n",
    "bins = np.arange(0,1,0.02)\n",
    "plt.hist([i for i in terminal_IOU if i>=0.5], bins=bins, color='red')\n",
    "plt.hist([i for i in terminal_IOU if i<0.5], bins=bins, color='blue')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,500)\n",
    "plt.axvline(average_terminal_IOU, color='black', label='MEAN: '+ str(average_terminal_IOU)[:5])\n",
    "plt.axvline(average_terminal_IOU-std_terminal_IOU, color='gray', linestyle='--', label='STDEV: '+ str(std_terminal_IOU)[:5])\n",
    "plt.axvline(average_terminal_IOU+std_terminal_IOU, color='gray', linestyle='--')\n",
    "plt.xlabel('IoU')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x75uBwvhuga4"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Get examples of images that did not have terminal actions\n",
    "# Get examples of images that had a terminal IOU below 0.5\n",
    "terminal_IOU_index = zip(terminal_index, terminal_IOU)\n",
    "false_pos_list = [i[0] for i in terminal_IOU_index if i[1] < 0.5]\n",
    "\n",
    "\n",
    "# Assessing the quality of the agent\n",
    "# look at cumulative reward as a function of steps \n",
    "# calculate the reward in testing with different models\n",
    "# calculate expected return\n",
    "\n",
    "\n",
    "IOU_difference = [[k-j for j,k in zip(i[:-1], i[1:])] for i in t2]\n",
    "\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "yx = np.vstack(all_image_centre).T\n",
    "y = yx[0,:]\n",
    "x = yx[1,:]\n",
    "z = list(np.vstack([i[-1] for i in all_IOU]).T[0])\n",
    "xi = np.linspace(x.min(), x.max(), x.max()-x.min()+1)\n",
    "yi = np.linspace(y.min(), y.max(), y.max()-y.min()+1)\n",
    "zi = griddata((x, y), z, (xi[None,:], yi[:,None]), method='cubic')\n",
    "\n",
    "zmin = 0.0\n",
    "zmax = 1.0\n",
    "zi[(zi<zmin)] = zmin\n",
    "zi[(zi>zmax)] = zmax\n",
    "\n",
    "cs = plt.contourf(xi, yi, zi, 15, cmap=plt.cm.rainbow, vmax=zmax, vmin=zmin)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYXcKwWyujXs"
   },
   "outputs": [],
   "source": [
    "# Log of parameters and testing scores\n",
    "log_names = ['class_file', 'Time_steps', 'termination_accuracy', \n",
    "\t\t\t'total_accuracy', 'precision', 'recall', 'F1', 'average_terminal_IOU',\n",
    "\t\t\t'average_TP_IOU', 'average_FP_IOU']\n",
    "\n",
    "log_vars = [class_file, T, termination_accuracy, total_accuracy, AP, Recall, F1, \n",
    "\t\t\taverage_terminal_IOU, average_TP_IOU, average_FP_IOU]\n",
    "\n",
    "log_location = project_root + 'project_code/network_weights/logs/'\n",
    "with open(log_location+saved_weights + '.csv', 'a') as csvfile:\n",
    "\tdetails = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\tdetails.writerow(log_names)\t\n",
    "\tdetails.writerow(log_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUbMTSwAupMu"
   },
   "source": [
    "# Video Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SQhJWOyusS3"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import collections\n",
    "\n",
    "\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "from keras.preprocessing import image\n",
    "\n",
    "### Local helpers\n",
    "import image_actions\n",
    "import reinforcement_helper\n",
    "import action_functions\n",
    "import image_loader\n",
    "\n",
    "\n",
    "### \n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9slNcZwNu1i6"
   },
   "outputs": [],
   "source": [
    "### Vars\n",
    "project_root = '/media/ersy/Other/Google Drive/QM Work/Queen Mary/Course/Final Project/'\n",
    "VOC_path = project_root+ 'Reinforcement learning/VOCdevkit/VOC2007'\n",
    "\n",
    "\n",
    "#image_path = \"/home/ersy/Downloads/aeroplane_example7.jpg\"\n",
    "#loaded_image = image.load_img(image_path, False)\n",
    "\n",
    "\n",
    "\n",
    "number_of_actions = 5\n",
    "history_length = 8\n",
    "Q_net_input_size = (25128, )\n",
    "\n",
    "\n",
    "### VGG16 model without top\n",
    "vgg16_conv = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "weights_path = '/media/ersy/Other/Google Drive/QM Work/Queen Mary/Course/Final Project/project_code/network_weights/final_weights/'\n",
    "\n",
    "# change the weights loaded for Q network testing\n",
    "saved_weights = 'Person_TEST.hdf5'\n",
    "weights = weights_path+saved_weights\n",
    "\n",
    "Q_net = reinforcement_helper.get_q_network(shape_of_input=Q_net_input_size, number_of_actions=number_of_actions, weights_path=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZ9_ulCmu408"
   },
   "outputs": [],
   "source": [
    "### Q network definition\n",
    "T = 60\n",
    "\n",
    "def detectObject(original_image, T):\n",
    "\t\"\"\"\n",
    "\ttakes in image as a numpy array, and a number of time steps then returns a localising bounding box around the object\n",
    "\t\"\"\"\n",
    "\t\n",
    "\timage_copy = np.copy(original_image)\n",
    "\timage_dimensions = image_copy.shape[:-1]\n",
    "\n",
    "\t# create the history vector\n",
    "\thistory_vec = np.zeros((number_of_actions, history_length))\n",
    "\n",
    "\t# preprocess the image\n",
    "\tpreprocessed_image = image_actions.image_preprocessing(original_image)\n",
    "\n",
    "\t# get initial state vector\n",
    "\tstate_vec = reinforcement_helper.get_state_as_vec(preprocessed_image, history_vec, vgg16_conv)\n",
    "\n",
    "\t# get initial bounding box\n",
    "\tboundingbox = np.array([[0,0],image_dimensions])\n",
    "\n",
    "\tall_proposals = []\n",
    "\n",
    "\tfor t in range(T):\n",
    "\t\t\t# add the current state to the experience list\n",
    "\t\t\tall_proposals.append(boundingbox)\n",
    "\n",
    "\t\t\t# plug state into Q network\n",
    "\t\t\tQ_vals = Q_net.predict(state_vec)\n",
    "\n",
    "\t\t\taction = np.argmax(Q_vals)\n",
    "\n",
    "\n",
    "\t\t\tif action != number_of_actions-1:\n",
    "\t\t\t\timage_copy, boundingbox = action_functions.crop_image(original_image, boundingbox, action)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"This is your object!\")\n",
    "\t\t\t\treturn boundingbox\n",
    "\t\t\t\t#break\n",
    "\n",
    "\n",
    "\t\t\t# update history vector\n",
    "\t\t\thistory_vec[:, :-1] = history_vec[:,1:]\n",
    "\t\t\thistory_vec[:,-1] = [0,0,0,0,0] # hard coded actions here\n",
    "\t\t\thistory_vec[action, -1] = 1\n",
    "\n",
    "\t\t\tpreprocessed_image = image_actions.image_preprocessing(image_copy)\n",
    "\t\t\tstate_vec = reinforcement_helper.get_state_as_vec(preprocessed_image, history_vec, vgg16_conv)\n",
    "\n",
    "\treturn all_proposals[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlvD4OVRu7nl"
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/media/ersy/Other/Google Drive/QM Work/Queen Mary/Course/Final Project/project_code/videos/Golf_Swing.mp4')\n",
    "frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "buf = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
    "\n",
    "fc = 0\n",
    "ret = True\n",
    "\n",
    "while (fc<205):#cap.read()[0]==True):\n",
    "    ret, buf[fc] = cap.read()\n",
    "    fc += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter('Golf_Swing.avi',fourcc, 24.0, (frameWidth, frameHeight), isColor=True)\n",
    "\n",
    "\n",
    "for frame in range(frameCount):\n",
    "\tprint(\"Frame: \", frame)\n",
    "\tcv2.namedWindow('frame',cv2.WINDOW_NORMAL)\n",
    "\tcv2.resizeWindow('frame', 600,600)\n",
    "\tbb = detectObject(buf[frame], 60)\n",
    "\tcv2.rectangle(buf[frame], (bb[0,1], bb[0,0]),(bb[1,1],bb[1,0]),(0,0,255),2)\n",
    "\t\n",
    "\tout.write(buf[frame])\n",
    "\tcv2.imshow('frame', buf[frame])\n",
    "\tcv2.waitKey(1)\n",
    "\n",
    "#cv2.namedWindow('frame 10')\n",
    "#cv2.imshow('frame 10', buf[9])\n",
    "\n",
    "#cv2.waitKey(0)\n",
    "out.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyDz7gPDunRe"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Tomato_Detection_Code | Veerbhadra.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
